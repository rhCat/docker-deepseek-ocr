# syntax=docker/dockerfile:1.6
########################
# Base toolchain layer #
########################
FROM nvidia/cuda:13.0.0-devel-ubuntu22.04 AS base

ENV DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC
RUN ln -fs /usr/share/zoneinfo/$TZ /etc/localtime && \
    apt-get update && apt-get install -y --no-install-recommends \
      git ca-certificates curl \
      python3 python3-pip python3-dev python3-venv python-is-python3 \
      build-essential cmake ninja-build pkg-config \
      libopenmpi-dev libjpeg-dev zlib1g-dev \
      protobuf-compiler libprotobuf-dev \
      tzdata python3-opencv && \
    dpkg-reconfigure -f noninteractive tzdata && \
    python3 -m pip install --upgrade pip setuptools wheel && \
    rm -rf /var/lib/apt/lists/*

############################
# 1) Fetch PyTorch sources #
############################
FROM base AS src-pytorch
ARG TORCH_REF=v2.6.0
WORKDIR /src/pytorch
RUN git clone --branch ${TORCH_REF} --depth 1 https://github.com/pytorch/pytorch . && \
    git submodule update --init --recursive
# ---- Patch vendored CMake files that demand older minima ----
# We bump them all to 3.5 so CMake >=3.25 stops erroring.
RUN set -eux; \
    for f in \
      third_party/psimd/CMakeLists.txt \
      third_party/FP16/CMakeLists.txt \
      third_party/FXdiv/CMakeLists.txt \
      third_party/cpuinfo/CMakeLists.txt \
      third_party/pthreadpool/CMakeLists.txt \
      aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt \
      third_party/protobuf/cmake/CMakeLists.txt \
    ; do \
      [ -f "$f" ] && sed -i '1s/.*/cmake_minimum_required(VERSION 3.5)/' "$f" || true; \
    done

#################################
# 2) Build wheel for PyTorch    #
#################################
FROM base AS build-pytorch
# (Optional) add OpenBLAS to avoid "no BLAS" warnings; not required for build
RUN apt-get update && apt-get install -y --no-install-recommends libopenblas-dev && rm -rf /var/lib/apt/lists/*
ENV USE_CUDA=1 USE_NCCL=1 USE_MKLDNN=0 USE_DISTRIBUTED=1 USE_CUDNN=0 \
    TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0;9.0a;12.1;12.1+PTX" \
    MAX_JOBS=8 \
    USE_SYSTEM_PROTOBUF=1 BUILD_CUSTOM_PROTOBUF=0
WORKDIR /build/pytorch
COPY --from=src-pytorch /src/pytorch/ ./
RUN python3 -m pip install -r requirements.txt
# Build WITHOUT passing CMAKE_ARGS (it leaked as -DCMAKE_ARGS=...)
# Install CUB headers system-wide (no cmake flags needed)
RUN git clone --depth 1 https://github.com/NVIDIA/cub.git /opt/cub && \
    mkdir -p /usr/local/include && \
    ln -s /opt/cub/cub /usr/local/include/cub && \
    ln -s /opt/cub/cub /usr/include/cub || true

# (optional but harmless) make a global include alias, in case the finder looks in /usr/include
RUN test -d /usr/local/cuda/include/cub && \
    ln -sf /usr/local/cuda/include/cub /usr/include/cub || true

# Add CUDA stub libs to linker/loader paths (ARM/Grace => sbsa-linux)
ENV CUDA_STUBS=/usr/local/cuda/targets/sbsa-linux/lib/stubs
ENV LD_LIBRARY_PATH=${CUDA_STUBS}:$LD_LIBRARY_PATH
ENV LIBRARY_PATH=${CUDA_STUBS}:$LIBRARY_PATH

# Preload a CMake policy script and pass the policy flag explicitly
RUN printf 'set(CMAKE_POLICY_VERSION_MINIMUM 3.5)\n' >/tmp/cmake-policy.cmake && \
    export CMAKE_ARGS="-C /tmp/cmake-policy.cmake -DCMAKE_POLICY_VERSION_MINIMUM=3.5" && \
    USE_NCCL=0 USE_DISTRIBUTED=0 USE_SYSTEM_CUB=1 CUB_INCLUDE_DIR=/usr/local/cuda/include CMAKE_POLICY_VERSION_MINIMUM=3.5 python3 setup.py bdist_wheel


RUN mkdir -p /out && cp dist/torch-2.6.0*.whl /out/


#################################
# 3) Fetch torchvision sources  #
#################################
FROM base AS src-vision
ARG VISION_REF=v0.21.0
WORKDIR /src/vision
RUN git clone --branch ${VISION_REF} --depth 1 https://github.com/pytorch/vision . 

#################################
# 4) Build wheel for torchvision#
#################################
# --- build vision wheel ---
FROM base AS build-vision
ENV FORCE_CUDA=1 CMAKE_ARGS="-DCMAKE_POLICY_VERSION_MINIMUM=3.5"
WORKDIR /build/vision
COPY --from=src-vision /src/vision/ ./

# Install the torch wheel built in the previous stage BEFORE building vision
COPY --from=build-pytorch /out/torch-2.6.0*.whl /tmp/
# OpenBLAS runtime needed when importing torch during the build
RUN apt-get update && apt-get install -y --no-install-recommends libopenblas0 && \
    rm -rf /var/lib/apt/lists/*
RUN TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0;9.0a;12.1;12.1+PTX" python3 -m pip install /tmp/torch-2.6.0*.whl

# now build torchvision
RUN TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0;9.0a;12.1;12.1+PTX" python3 setup.py bdist_wheel
RUN mkdir -p /out && cp dist/torchvision-0.21.0*.whl /out/

#################################
# 5) Runtime image (app + deps) #
#################################
FROM nvidia/cuda:13.0.0-runtime-ubuntu22.04 AS runtime

ENV PYTHONUNBUFFERED=1 \
    HF_HOME=/cache/hf \
    TRANSFORMERS_CACHE=/cache/hf \
    TORCH_HOME=/cache/torch \
    TORCH_ALLOW_TF32=1 \
    PORT=8100 \
    DEBIAN_FRONTEND=noninteractive \
    TZ=Etc/UTC

# --- OS deps FIRST (so torch can import later) ---
RUN ln -fs /usr/share/zoneinfo/$TZ /etc/localtime && \
    apt-get update && apt-get install -y --no-install-recommends \
      python3 python3-pip python-is-python3 \
      tzdata git \
      libopenblas0 libnuma1 \
      cuda-cupti-12-4 \
      python3-opencv \
    && dpkg-reconfigure -f noninteractive tzdata \
    && echo "/usr/local/cuda/extras/CUPTI/lib64" > /etc/ld.so.conf.d/cupti.conf \
    && ldconfig \
    && python3 -m pip install --upgrade pip setuptools wheel \
    && rm -rf /var/lib/apt/lists/*

ENV OPENBLAS_NUM_THREADS=1 \
    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:${LD_LIBRARY_PATH}

# --- Install the built wheels (torch/vision) ---
COPY --from=build-pytorch /out/torch-2.6.0*.whl /tmp/
COPY --from=build-vision  /out/torchvision-0.21.0*.whl /tmp/
RUN python3 -m pip install /tmp/torch-2.6.0*.whl /tmp/torchvision-0.21.0*.whl \
    && rm -f /tmp/*.whl

# --- Python deps AFTER torch/vision ---
RUN python3 -m pip install \
      transformers==4.46.3 tokenizers==0.20.3 \
      einops addict easydict python-multipart \
      safetensors sentencepiece fastapi uvicorn pillow peft \
    && python3 -m pip install flash-attn==2.7.3 --no-build-isolation || true

WORKDIR /app
COPY app.py /app/

EXPOSE 8100
CMD ["python3", "-m", "uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8100"]

